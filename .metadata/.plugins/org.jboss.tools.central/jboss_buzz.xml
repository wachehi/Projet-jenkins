<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Test container images in Red Hat OpenShift 4 with Ansible and CI/CD</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bljgoDky4Wg/test-container-images-red-hat-openshift-4-ansible-and-cicd" /><author><name>Petr Hracek</name></author><id>006c2cb0-68fe-4e13-8f71-d003e718625e</id><updated>2021-08-13T07:00:00Z</updated><published>2021-08-13T07:00:00Z</published><summary type="html">&lt;p&gt;Several repositories offer ready-made &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images for &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; and other systems running &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;. The InterOp team at Red Hat tests these application images in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. To simplify the integration of tests into the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous delivery (CI/CD)&lt;/a&gt; process, we are adding &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; playbooks to the repositories that host the container images. The Red Hat Software Collections &lt;a href="https://github.com/sclorg/"&gt;GitHub repository&lt;/a&gt; currently has the first of these Ansible playbooks, but we will add playbooks to other repositories over time.&lt;/p&gt; &lt;p&gt;This article shows how to submit a test to a repository, and how to download the tests if you want to run them in your own container environment.&lt;/p&gt; &lt;h2&gt;Parameters for testing a container&lt;/h2&gt; &lt;p&gt;In order to test a container under a Red Hat OpenShift 4 environment, the developer has to provide information about where to download, deploy, and test the container. The necessary information is illustrated in the following playbook for a PostgreSQL container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;registry_redhat_io: "rhscl/postgresql-10-rhel7" tag_name: "postgresql:10-el7" deployment: "oc new-app postgresql:10-el7~https://github.com/sclorg/postgresql-container.git \ --name new-postgresql \ --context-dir examples/extending-image/ \ -e POSTGRESQL_USER=user \ -e POSTGRESQL_DATABASE=db \ -e POSTGRESQL_PASSWORD=password" pod_name: "new-postgresql" add_route: true test_exec_command: "./files/check_postgresql_container.sh" expected_exec_result: "FINE" check_curl_output: “SOMETHING from curl output” scl_url: "postgresql-container" is_name: "postgresql"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The meanings of the fields follow:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;registry_redhat_io&lt;/code&gt;: The image in the registry.redhat.io catalog, including the namespace, which is &lt;code&gt;rhscl&lt;/code&gt; in this case.&lt;/li&gt; &lt;li&gt;&lt;code&gt;tag_name&lt;/code&gt;: The tag name of the image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;deployment&lt;/code&gt;: The command that deploys the image into the OpenShift environment.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pod_name&lt;/code&gt;: name of the pod in the OpenShift namespace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;add_route&lt;/code&gt;: Whether the route should be exposed, where the default is not to expose it.&lt;/li&gt; &lt;li&gt;&lt;code&gt;test_exec_command&lt;/code&gt;: The file that performs the test.&lt;/li&gt; &lt;li&gt;&lt;code&gt;expected_exec_result&lt;/code&gt;: A string expected from executing the &lt;code&gt;test_exec_command&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;check_curl_output&lt;/code&gt;: A substring from the expected output of a &lt;code&gt;curl&lt;/code&gt; command.&lt;/li&gt; &lt;li&gt;&lt;code&gt;scl_url&lt;/code&gt;: The repository name, for the Software Collections repositories only.&lt;/li&gt; &lt;li&gt;&lt;code&gt;is_name&lt;/code&gt;: The imagestream of the container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To submit a test for public use, file a pull request at the site hosting the tests. There is currently one site at the &lt;a href="https://github.com/sclorg/ansible-tests/pulls"&gt;Software Collections GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Storing a test locally&lt;/h2&gt; &lt;p&gt;If you want to keep the test in your private environment instead of sharing the test, you can download our test suite and add your test to it as follows.&lt;/p&gt; &lt;p&gt;Clone the test repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/sclorg/ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add your container test suite to the &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/deploy-and-test.yml"&gt;main Ansible playbook&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Running a test&lt;/h2&gt; &lt;p&gt;This section assumes that you are running an OpenShift 4 cluster.&lt;/p&gt; &lt;h3&gt;Downloading the OpenShift 4 client&lt;/h3&gt; &lt;p&gt;The latest version of the OpenShift 4 client, 4.6.18, can be obtained for your system at &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.618/"&gt;this mirror site&lt;/a&gt;. Download the ZIP file and unpack it through:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tar -xzvf &lt;FILE&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The kubeconfig file&lt;/h3&gt; &lt;p&gt;Tests refer to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"&gt;kubeconfig&lt;/a&gt; file, so you need to point the &lt;code&gt;KUBECONFIG&lt;/code&gt; environment variable to the file. Ask your OpenShift 4 cluster administrator for the location of the file, then insert the path into the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KUBECONFIG=&lt;path_to_kubeconfig&gt;/kubeconfig&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Running your test&lt;/h3&gt; &lt;p&gt;Switch to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the test as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ make ocp4-tests EXT_TEST=&lt;your_test_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details are available in a &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/README_ocp4.md"&gt;README file&lt;/a&gt; in the repository.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article showed a new method that makes it easy to add a test for your container to a test suite and run the test in an OpenShift 4 cluster that has been set up by an administrator. You can keep the test in your own environment, but we recommend that you heed the Red Hat phrase, "It’s better to share."&lt;/p&gt; &lt;p&gt;By providing your test to the InterOp team, you can get the container tested during each feature freeze or code freeze done by the OpenShift 4 development team. Feel free to &lt;a href="https://issues.redhat.com/projects/LPINTEROP/issues/LPINTEROP-1858?filter=allopenissues"&gt;contact the InterOp team at Red Hat&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd" title="Test container images in Red Hat OpenShift 4 with Ansible and CI/CD"&gt;Test container images in Red Hat OpenShift 4 with Ansible and CI/CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bljgoDky4Wg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Petr Hracek</dc:creator><dc:date>2021-08-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd</feedburner:origLink></entry><entry><title type="html">How to Capture Business Decisions using DMN: Introduction to Some Basic Patterns and Their Value</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/DZN0Qbu7ROo/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2021/08/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html</id><updated>2021-08-12T08:50:00Z</updated><content type="html">I am very glad for the opportunity to have with Denis Gagné CEO &amp;amp; CTO of Trisotech! 1. how business analysts can use the DMN open-standard to capture the requirements for operational business decisions 2. some of the recurring basic patterns in modeling (Q&amp;amp;A, Scoring, Classification and Categorisation, Ranking..) 3. how to transform these decision models into actual In the following you can find the recording, as well as a brief summary of some key highlights for the patterns. WEBINAR RECORDING IIBA Webinar recording Businesses continuously make Business Decisions. Some of these decisions are strategic business decisions, but a lot are operational business decisions taken every day within every transaction. With the ever-increasing number of laws and regulations that may apply or regulate these operational business decisions, business analysts are more often called upon to document/specify how these business decisions are to be taken in order to provide transparency and to offer auditable traces of the actual decisions taken. In this insightful session, we will introduce how business analysts can use DMN to capture the requirements for operational business decisions, some of the recurring basic patterns in modeling these business decisions and will even show how to transform these decision models into actual executable business decision services. PATTERN: Q&amp;amp;A Not to be confused with the generally applicable multiple-choice paradigm, on of the key aspects of DMN models is that each Decision is meant to answer a question, usually a business related question or a domain question. This Q&amp;amp;A pattern is a built-in of the DMN standard, thanks to the optional attributes "question" and "allowedAnswers", which business analyst and subject matter expert modelers often use to describe using natural language to complement the modeled decision logic semantic. PATTERN: SCORING Input variables, described as InputData, are often analysed and weighted into a score, typically to be later used in the DMN model with other patterns (such as categorisation) or directly with decision logic such as thresholds. Examples: PATTERN: CLASSIFICATION In this pattern, model variables are recognized, differentiated and classified to be better understood; usually this is done to verbalise classes and often using a Score as an input (ref above). It is important to be aware of several types of classification: * Classification: separating based on class labels * Clustering: separating based similarities without class labels * Categorization: subsuming classes (to realize a taxonomy) * Segmentation: complete and disjunct categorization Examples: PATTERN: RANKING In this pattern, the position, or rank, of each item in a collection is determined. This pattern is usually a bit more complex to implement if compared to the previous one, requiring iteration with sorting. Fortunately the DMN standard gives all the tools to implement this pattern more easily! For example: TRANSFORM DMN MODELS INTO EXECUTABLE BUSINESS DECISION SERVICES Have you found this content interesting? Don’t forget a great advantage of DMN models is that they can be immediately deployed as executable services, thanks to the and ! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/DZN0Qbu7ROo" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html</feedburner:origLink></entry><entry><title>Build and deploy microservices with Kubernetes and Dapr</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/sRPE9yNLZBM/build-and-deploy-microservices-kubernetes-and-dapr" /><author><name>Ip Sam</name></author><id>e3e788c4-5230-41c1-9cb2-0168445a6352</id><updated>2021-08-12T07:00:00Z</updated><published>2021-08-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://dapr.io/"&gt;Dapr&lt;/a&gt; (Distributed Application Runtime) provides an event-driven, portable runtime for building distributed &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;. The project is useful for both stateless or stateful applications on the cloud and at the network edge. A new &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project from Microsoft, Dapr embraces a diversity of languages and development frameworks. The project is a natural fit for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to install Dapr and walks you through the process of building a sample application on Kubernetes.&lt;/p&gt; &lt;h2&gt;Dapr components&lt;/h2&gt; &lt;p&gt;The basic architecture of Dapr consists of &lt;em&gt;building blocks&lt;/em&gt;, which in turn contain &lt;em&gt;components&lt;/em&gt; (Figure 1). The building blocks offer distributed system capabilities such as publications and subscriptions, state management, resource bindings, and distributed tracing. Each building block exposes a public &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API&lt;/a&gt; that is called from your code. Table 1 describes the different types of Dapr building blocks.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_bb.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_bb.png?itok=8bNmOMNu" width="522" height="591" alt="Dapr offers APIs through building block, each containing multiple components to implement the APIs." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Components in a Dapr building block. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1. Dapr building blocks.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Component &lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Function&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Service-to-service invocation&lt;/td&gt; &lt;td&gt;Perform direct, secure, service-to-service method calls.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;State management&lt;/td&gt; &lt;td&gt;Create long-running stateless and stateful services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Publish and subscribe&lt;/td&gt; &lt;td&gt;Provide secure and scalable messaging between services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Resource bindings and triggers&lt;/td&gt; &lt;td&gt;Trigger code through events from a large array of input. Output binding to external resources including databases and queues.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Actors&lt;/td&gt; &lt;td&gt;Encapsulate code and data in reusable actor objects as a common micro-service design pattern.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Observability&lt;/td&gt; &lt;td&gt;See and measure the message calls across components and network services.&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Secrets&lt;/td&gt; &lt;td&gt;Securely access secrets from your applications.&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Components encapsulate the implementations for a building block's API. Components include Ceph, PostgreSQL, MySQL, Redis, and MongoDB. Many of the components are pluggable, so that one implementation can be swapped out for another. Each component has an interface definition.&lt;/p&gt; &lt;p&gt;The building blocks integrate with application code and different cloud service providers, as shown in Figure 2. The architecture for Dapr on Red Hat OpenShift and Kubernetes is shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_integration.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_integration.png?itok=dbqc0ipl" width="600" height="289" alt="Dapr can work with multiple platforms." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Dapr architecture with cloud providers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_kubernetes.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_kubernetes.png?itok=5hwfVCdj" width="600" height="287" alt="Dapr integrates as pods with Kubernetes and OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Dapr components with Kubernetes and Red Hat OpenShift.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How to install Dapr&lt;/h2&gt; &lt;p&gt;Follow these steps in this section to install Dapr in your OpenShift cluster.&lt;/p&gt; &lt;p&gt;First, download Dapr from GitHub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Log in to your OpenShift cluster as an administrator. Check your login status:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc whoami&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are using a Helm repository to install Dapr, you need to add the Dapr URL to the Helm repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo add dapr https://daprio.azurecr.io/helm/v1/repo "dapr" has been added to your repositories&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you can update the Helm repository to get the latest changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ash"&gt;$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the "dapr" chart repository Update Complete. ⎈ Happy Helming!⎈&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new namespace in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create namespace dapr-system namespace/dapr-system created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install Dapr to the new namespace that you just created:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm install dapr dapr/dapr --namespace dapr-system NAME: dapr LAST DEPLOYED: Mon Apr 6 12:48:40 2020 NAMESPACE: dapr-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Dapr: High-performance, lightweight serverless runtime for cloud and edge Your release is named dapr. To get started with Dapr, we recommend using our samples page: https://github.com/dapr/samples For more information on running Dapr, visit: https://dapr.io&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following Dapr pods will be created:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;dapr-operator:&lt;/code&gt; Manages components and services endpoints for Dapr (state stores, pub-subs, etc.).&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-sidecar-injector&lt;/code&gt;: Injects Dapr into annotated pods.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-placement&lt;/code&gt;: Used for actors only. Creates mapping tables that map actor instances to pods.&lt;/li&gt; &lt;li&gt;&lt;code&gt;dapr-sentry&lt;/code&gt;: Manages transport layer security and acts as a certificate authority.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Check your pods' status to make sure that they are in the running state:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods -n dapr-system -w NAME READY STATUS RESTARTS AGE dapr-operator-7d9668fd8b-skmbh 1/1 Running 0 68s dapr-placement-7dbcc6bf59-zxx4s 1/1 Running 0 68s dapr-sentry-756f7799fd-xm57l 1/1 Running 0 68s dapr-sidecar-injector-7849f77c4b-l789t 1/1 Running 0 68s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Build a sample application in Node.js&lt;/h2&gt; &lt;p&gt;This section shows how to get Dapr running in Red Hat OpenShift and deploy a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application that handles retail orders. Another application, written in &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, generates messages containing the orders. The Node.js application subscribes to messages and persists them, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_endpoints.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_endpoints.png?itok=RcZOHBGe" width="600" height="330" alt="The example application offers GET and POST endpoints, mediated by Dapr, and persistent state stores." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: GET and POST endpoints, mediated by Dapr, and persistent state stores. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 5 shows the Dapr components for the producer and consumer applications.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dapr_apps.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dapr_apps.png?itok=zW_qkAt2" width="600" height="305" alt="The Dapr APIs communicate with the example's two apps." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Dapr and the example's two apps. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The implementation uses Redis as a state store for data persistence.&lt;/p&gt; &lt;p&gt;To access the code, clone the following repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/dapr/samples.git $ cd samples/1.hello-world&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; code for the order POST endpoint follows. The application persists the order information in Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;app.post('/neworder', (req, res) =&gt; { const data = req.body.data; const orderId = data.orderId; console.log("Got a new order! Order ID: " + orderId); const state = [{ key: "order", value: data }]; fetch(stateUrl, { method: "POST", body: JSON.stringify(state), headers: { "Content-Type": "application/json" } }) .then((response) =&gt; { if (!response.ok) { throw "Failed to persist state."; } console.log("Successfully persisted state."); res.status(200).send(); }) .catch((error) =&gt; { console.log(error); res.status(500).send({message: error}); }); }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The code for the order GET endpoint follows. The application retrieves the latest order information from Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;app.get('/order', (_req, res) =&gt; { fetch(`${stateUrl}/order`) .then((response) =&gt; { if (!response.ok) { throw "Could not get state."; } return response.text(); }).then((orders) =&gt; { res.send(orders); }).catch((error) =&gt; { console.log(error); res.status(500).send({message: error}); }); }); &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the application on Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;Install Redis in OpenShift from a Helm chart as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ helm repo add bitnami https://charts.bitnami.com/bitnami "bitnami" has been added to your repositories $ helm install redis bitnami/redis NAME: redis LAST DEPLOYED: Mon Apr 6 12:58:12 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Extract the secret from the default namespace for Redis:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get secret --namespace default redis -o jsonpath="{.data.redis-password}" | base64 --decode pbOe0CgPsu&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now update the &lt;code&gt;redis.yaml&lt;/code&gt; file in the deployment directory. Change &lt;code&gt;redisHost&lt;/code&gt; to &lt;code&gt;redis-master:6379&lt;/code&gt; and &lt;code&gt;redisPassword&lt;/code&gt; to the value extracted in the previous step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: statestore spec: type: state.redis metadata: - name: redisHost value: redis-master:6379 - name: redisPassword value: Pnkkpan9gs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the Redis resource in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f deploy/redis.yaml component.dapr.io/statestore created &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create the Node.js and Python applications:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f deploy/node.yaml service/nodeapp created deployment.apps/nodeapp created $ oc apply -f deploy/python.yaml deployment.apps/pythonapp created &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To examine your Dapr pods in OpenShift, you can run the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods -n dapr-system -w NAME READY STATUS RESTARTS AGE dapr-operator-7c6799878d-v4stm 1/1 Running 14 54m dapr-placement-76c99b79bb-n6sfw 1/1 Running 0 54m dapr-sentry-5644b86cf9-8fjpv 1/1 Running 0 54m dapr-sidecar-injector-84c5578f8d-d6dp4 1/1 Running 0 54m nodeapp-548959b4b9-5rdnl 2/2 Running 0 21m pythonapp-79c9b55c8f-p7gng 2/2 Running 0 18m redis-master-0 1/1 Running 0 52m redis-slave-0 1/1 Running 1 52m redis-slave-1 1/1 Running 0 49m &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can test the Dapr application by viewing the logs coming out from the pod and looking at what is consuming the messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs pod/nodeapp-548959b4b9-5rdnl -c node Node App listening on port 3000! Got a new order! Order ID: 1 Successfully persisted state Got a new order! Order ID: 2 Successfully persisted state Got a new order! Order ID: 3 Successfully persisted state&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also expose a route from the &lt;code&gt;nodeapp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc expose svc nodeapp route.route.openshift.io/nodeapp exposed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, invoke the &lt;code&gt;nodeapp&lt;/code&gt; order endpoint to confirm the successful persistence:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl nodeapp-default.apps-crc.testing/order {"orderID":"42"} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You have just successfully deployed a Dapr application on OpenShift. You might want to update the sample code to fit your scenario by forking the &lt;a href="https://github.com/dapr/samples.git"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Dapr with Kubernetes or Red Hat OpenShift enables the easy development of event-driven, stateful microservices. Dapr also provides consistency and portability via standard open APIs. It is an open source project that works well with numerous programming languages and development frameworks.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/12/build-and-deploy-microservices-kubernetes-and-dapr" title="Build and deploy microservices with Kubernetes and Dapr"&gt;Build and deploy microservices with Kubernetes and Dapr&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/sRPE9yNLZBM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ip Sam</dc:creator><dc:date>2021-08-12T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/12/build-and-deploy-microservices-kubernetes-and-dapr</feedburner:origLink></entry><entry><title type="html">Observability in Hybrid Multi-cloud environment</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_T3Q4nTaGUQ/observability-in-hybrid-multi-cloud.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/67AwzMkvBKc/observability-in-hybrid-multi-cloud.html</id><updated>2021-08-12T02:48:00Z</updated><content type="html">QUICK RECAP,  Last article I talked about the study I did among Red Hat customers that makes the jump towards deploying their workloads on hybrid and multi-cloud environments.  These articles are abstractions of the common generic components summarized according to the actual implementations.  To overcome the common obstacles of going hybrid and multi-cloud, such as finding talents with multi-cloud knowledge. Secure and protect across low trust networks or just day to day operation across the board. I have identify some solutions from the study, where I will be covering in the serie of articles:  * *   * * Customize monitoring and logging strategy, One of the keys to manage multiple clusters is observability. In a single cluster environment, metrics and logs are segregated in different layers, application, cluster components and nodes, now adding in the complexity of multiple clusters on different clouds will definitely make it more chaotic than ever. By customizing how to gather and view the metrics allows you to be more effective, easier to operate, pinpoint and locate problems quickly. To do that, we will need to decide how we want to aggregate the collected data, by regions, operations or a centralized point. (Depends on your HA strategy and how much data you are collecting).  And then decide how to scrape or collect the metrics and logs. Historical data are valuable not only for us to view and identify problems. Since many vendors now support AI based remediation, by validating and extracting the data for operational models. Therefore we will also need to persist data. HOW IT WORKS,  Checkout my previous article on the setup of the hybrid and multi cloud environment, and if you are interested, other articles on GitOps and secure dynamic infrastructure. But this time, let’s look at how observability works. First to view things in a single pane of glass, we host Grafana dashboard to stream and query the Observatorium service in all managed clusters.  When bootstrapping the managed clusters on each cloud, we will need to install the following,  For Monitoring,  Prometheus is installed to scrape metrics for cluster components, as well as the applications. A Thanos sidecar gets deployed along with a Prometheus instance to persist metrics to storage and allow instant query to Prometheus data. You may have multiple Prometheus instance per cluster depends on how you want to distribute the workload.  Observatorium is installed based on your defined strategy for observability(in this case, per region), this will deploy a set of service instances on the cluster. Where they will be responsible for aggregating, storing input metrics, as well as efficiently storing the data. Also providing endpoints(API) for observation tools like Grafana to query the persisted data.  1. Queries from the Grafana dashboard in Hub cluster, the central Querier component in Observatorium process the PromQL queries and aggregate the results.  2. Prometheus scraps metrics in the local cluster, Thano sidecar pushes metrics to Observatorium to persist in storage.  3. Thanos sidecar acts as a proxy that serves Prometheus’s local data over Thanos’s gRPC API from the Querier.  For Logging,  Promtail collects log files with fine-grained control of what to ingest, what to drop, and the final metadata to attach to the log line. Similar to Prometheus, multiple Promtail instances can be installed per cluster depending on how you want to distribute the logging workload.  Observatorium in each defined region are also configured with Grafana Loki, where it aggregates logs by labeling each log stream pushed from Promtail. Not only it persist logs in stooges but also allow you to query the high cardinality data for better analysis and  visualization.  1. Promtail is used to collect log and push to Loki API (Observatorium) 2. In Observatorium, Loki distributor sends logs in batches to ingester, where they will be persisted. Couple of things to beware both ingester and querier requires large memory consumption, will need more replicas 3. Grafana dashboard in Hub cluster display logs via requesting  1. Real time display(tail) with websocket 2. Time-series base query with HTTP This summarizes how observability was implemented in our customer’s hybrid multi-cloud environment. If you want to dive deeper into the technology, check out a video done by Ales Nosek.  &lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_T3Q4nTaGUQ" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/67AwzMkvBKc/observability-in-hybrid-multi-cloud.html</feedburner:origLink></entry><entry><title>Simplify load balancing for API gateways using Red Hat 3scale API Management</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XCdhz42ERK4/simplify-load-balancing-api-gateways-using-red-hat-3scale-api-management" /><author><name>Srikanth Valluru</name></author><id>0872b974-13eb-476c-80f2-5f89d53baa57</id><updated>2021-08-11T07:00:00Z</updated><published>2021-08-11T07:00:00Z</published><summary type="html">&lt;p&gt;One of the conveniences offered by &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management&lt;/a&gt; is simplified load balancing on &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API&lt;/a&gt; gateways. This article shows how to use a route on &lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and 3scale API Management to load balance two API gateways installed on an OpenShift instance.&lt;/p&gt; &lt;p&gt;Figure 1 shows the architecture used in this article. This setup is only one of several ways to deploy API gateways. Examples of setups that are not discussed in this article include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Deploy API gateways inside your corporate network closer to your API, and place a load balancer (registered with 3scale API Management) in your demilitarized zone (DMZ) to handle load balancing, allowing high performance and avoiding a single point of failure. This architecture hides your API gateway from the outside world.&lt;/li&gt; &lt;li&gt;Install the API gateways via Docker containers along with a load balancer that forwards requests to the Docker containers.&lt;/li&gt; &lt;li&gt;Use Kubernetes scaling in OpenShift to run the desired number of API gateway pods and have OpenShift’s route feature take care of load balancing.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_architecture_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_architecture_0.png?itok=2EGEUzD-" width="600" height="294" alt="In the architecture used for this article, OpenShift contains a load balancer that splits requests between two services provided by two API gateways." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Load balancer between the 3scale API Management and the API gateways.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before starting the procedure in this article, you need to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Set up 3scale API Management on either SaaS or Red Hat OpenShift.&lt;/li&gt; &lt;li&gt;Create a back-end product for the default staging and production API gateways.&lt;/li&gt; &lt;li&gt;Set up a project (namespace) on OpenShift where you'll install the 3scale API gateway operator.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;In my configuration, I used the SaaS version for 3scale API Management with an Echo API.&lt;/p&gt; &lt;h2&gt;Set up the load balancer&lt;/h2&gt; &lt;p&gt;The rest of this article sets up load balancing by taking you through the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Install the 3scale APIcast gateway operator.&lt;/li&gt; &lt;li&gt;Deploy one instance of a self-managed API gateway.&lt;/li&gt; &lt;li&gt;Deploy a second instance of a self-managed API gateway.&lt;/li&gt; &lt;li&gt;Configure an OpenShift route for the load balancer.&lt;/li&gt; &lt;li&gt;Configure a 3scale API Management product using the load balancer route's URL.&lt;/li&gt; &lt;li&gt;Promote your product API to use the load balancer route's URL in the staging API gateway.&lt;/li&gt; &lt;li&gt;Test the API.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The following video also walks through the steps. As it shows, the whole process can take less than ten minutes.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h3&gt;Step 1: Install the 3scale APIcast gateway operator&lt;/h3&gt; &lt;p&gt;From the OpenShift Operator console, install the &lt;strong&gt;Red Hat Integration&lt;/strong&gt; - &lt;strong&gt;3scale APIcast gateway&lt;/strong&gt; operator (Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_operator.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_operator.png?itok=DsNlH6zV" width="266" height="252" alt="The 3scale APIcast gateway Operator is available in the OpenShift console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: 3scale APIcast gateway operator in the OpenShift console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 2: Deploy one instance of a self-managed API gateway&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html/installing_3scale/installing-apicast#deploying-apicast-gateway-self-managed-operator"&gt;"Deploying an APIcast gateway self-managed solution using the operator"&lt;/a&gt; section of the 3scale API Management documentation describes this step in detail.&lt;/p&gt; &lt;h3&gt;Step 3: Deploy a second instance of a self-managed API gateway&lt;/h3&gt; &lt;p&gt;The procedure is just like installing the first instance, except that you should use a different secret. In other words, the secret shown in the following configuration parameter must be different in the two instances:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;spec: adminPortalCredentialsRef: name: SOME_SECRET_NAME&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second instance of a self-managed API gateway in the same project (namespace) involves a slight change in how the secret is created.&lt;/p&gt; &lt;p&gt;At this point, you should see the two API gateways in the console (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_instance_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_instance_0.png?itok=3_tLONZZ" width="600" height="162" alt="Two instances are shown under 3scale APIcast gateway in the console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: 3scale APIcast gateways in the console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There are also two pods running, one for each API gateway (Figure 4).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_pods.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_pods.png?itok=tu3n8ASD" width="600" height="212" alt="Two pods are running 3scale APIcast gateways." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: 3scale APIcast pods in the console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 4: Configure an OpenShift route for the load balancer&lt;/h3&gt; &lt;p&gt;There are many ways to configure a load balancer. This section uses the OpenShift route to split traffic between services.&lt;/p&gt; &lt;p&gt;Figure 5 shows the services for the API gateways you created previously.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_pods_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_pods_0.png?itok=jOKNoBzF" width="600" height="212" alt="Screenshot showing two services listed in the 3scalegateway project: apicast-example2-apicast and apicast-example-apicast." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The services listed in the &lt;code&gt;3scalegateway&lt;/code&gt; project.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While creating the route, configure the two services with equal service weight (Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_lb_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_lb_0.png?itok=K1OIZNXn" width="600" height="647" alt="The load balancer route has its own configuration." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Configuration for the load balancer route.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 5: Configure a 3scale API Management product using the load balancer route's URL&lt;/h3&gt; &lt;p&gt;Go to &lt;strong&gt;3scale API Management&lt;/strong&gt; → &lt;strong&gt;Products&lt;/strong&gt; → &lt;strong&gt;&lt;Product Name&gt;&lt;/strong&gt; → &lt;strong&gt;Integration&lt;/strong&gt; → &lt;strong&gt;Settings&lt;/strong&gt;. Copy the route URL from Step 4 and set the &lt;strong&gt;Staging Public Base URL&lt;/strong&gt; and &lt;strong&gt;Production Public Base URL&lt;/strong&gt; in 3scale API Management (Figure 7).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_product_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_product_0.png?itok=CAFOUMqx" width="600" height="332" alt="The product has its own configuration." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: 3scale API Management configuration.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 6: Promote your product API to use the load balancer route's URL in the staging API gateway&lt;/h3&gt; &lt;p&gt;Go to &lt;strong&gt;3scale API Management&lt;/strong&gt; → &lt;strong&gt;Products&lt;/strong&gt; → &lt;strong&gt;&lt;Product Name&gt;&lt;/strong&gt; → &lt;strong&gt;Integration&lt;/strong&gt; → &lt;strong&gt;Configuration&lt;/strong&gt; and promote the Staging APIcast configuration (Figure 8).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_apicast.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_apicast.png?itok=nV2DjLOY" width="600" height="305" alt="The APIcast has its own configuration." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Configuration for the APIcast.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 7: Test the API&lt;/h3&gt; &lt;p&gt;The APIcast Configuration page shown in Figure 8 contains a cURL command that you can use to test the service. Repeat it to fire up a few requests. You should then see requests sent to both API gateways in a round-robin fashion. Figure 9 highlights the parts of the output that show how both API gateways are responding.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apigateway_curl.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/apigateway_curl.png?itok=DO6_Uq44" width="600" height="445" alt="Information retrieved by cURL commands to the service show that two different hosts are responding." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Information retrieved by cURL commands to the service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;3scale API Management supports multiple deployment strategies with API gateways. For more information, see the following links:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html/installing_3scale/installing-apicast"&gt;Installing APIcast&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html/installing_3scale/running-apicast-on-red-hat-openshift"&gt;Running APIcast on Red Hat Openshift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html/installing_3scale/deploying-apicast-on-the-docker-containerized-environment"&gt;Deploying APIcast on the Docker containerized environment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html/installing_3scale/deploying-apicast-on-podman"&gt;Deploying APIcast on Podman&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/11/simplify-load-balancing-api-gateways-using-red-hat-3scale-api-management" title="Simplify load balancing for API gateways using Red Hat 3scale API Management"&gt;Simplify load balancing for API gateways using Red Hat 3scale API Management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XCdhz42ERK4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Srikanth Valluru</dc:creator><dc:date>2021-08-11T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/11/simplify-load-balancing-api-gateways-using-red-hat-3scale-api-management</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 11 August 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/vXVnnsrA7Uk/weekly-2021-08-11.html" /><category term="Quarkus" /><category term="DMN" /><category term="Drools" /><category term="Quarkys" /><category term="Hybrid-cloud" /><category term="kie" /><author><name>Romain Pelisse</name><uri>https://www.jboss.org/people/romain-pelisse</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-08-11.html</id><updated>2021-08-11T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="Quarkus, DMN, Drools, Quarkys, Hybrid-cloud, kie"&gt; &lt;h1&gt;This Week in JBoss - 11 August 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;After short pause for the summer holiday, welcome back to this new installment of the JBoss Editorial! As always, the JBoss community has been quite active and there is quite a lot to cover today, especially on hybrid cloud and Kogito. Let’s dive right in!&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_hybrid_cloud"&gt;Hybrid Cloud&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;During the last weeks, Christina Lei did a quite interesting series of articles on hybrid cloud. After an &lt;a href="https://wei-meilin.blogspot.com/2021/08/overview-of-making-hybrid-multi-cloud.html"&gt;overview of making hybrid cloud&lt;/a&gt;, she explores some in-depth topic like &lt;a href="https://wei-meilin.blogspot.com/2021/08/hybrid-multi-cloud-dynamic-security.html"&gt;dynamic security&lt;/a&gt; or &lt;a href="https://wei-meilin.blogspot.com/2021/08/observability-in-hybrid-multi-cloud"&gt;observability&lt;/a&gt;. To conclude this serie, another article covers &lt;a href="http://wei-meilin.blogspot.com/2021/08/a-study-of-hosting-and-managing-on.html"&gt;hosting and managing on hybrid cloud&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_kogito_this_and_kogito_that"&gt;Kogito this and Kogito that!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;With the new release of &lt;a href="https://blog.kie.org/2021/08/kogito-1-9-1-released.html"&gt;Kogito 1.9.1&lt;/a&gt;, it’s time to explore (maybe again) all the potential of this framework. And fortunately, there are a lot of in-depth articles released recently doing just that! Let’s start with one on &lt;a href="https://blog.kie.org/2021/08/how-to-capture-business-decisions-using-dmn-introduction-to-some-basic-patterns-and-their-value.html"&gt;how to capture decisions using DMN&lt;/a&gt; followed by a tutorial on how to implement &lt;a href="https://blog.kie.org/2021/08/kogito-task-deadlines.html"&gt;task’s deadline&lt;/a&gt; with Kogito.&lt;/p&gt; &lt;p&gt;Once you get the hang of it, you’ll want to learn how to manage your &lt;a href="https://blog.kie.org/2021/08/four-steps-to-author-bpmn-and-dmn-assets-on-gitpod-io.html"&gt;BPMN and DMN assets on gitpod.io&lt;/a&gt;. Or explore &lt;a href="https://blog.kie.org/2021/08/event-driven-decisioning-with-amq-streams-and-kogito.html"&gt;event-driven decisinning with AMQ streams and Kogito&lt;/a&gt;. If those articles have but open your appetite for the framework, you can carry on with a tutorial on &lt;a href="https://blog.kie.org/2021/08/how-develop-better-widgets-with-showcase-appl.html"&gt;how to develop better widgets&lt;/a&gt; or how to &lt;a href="https://blog.kie.org/2021/08/add-data-from-kie-execution-server-for-authoring-dashboards.html"&gt;add data from KIE server for authoring dashboards&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_techbytes"&gt;Techbytes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Want to learn more? Don’t worry, we have few more intriguing technical articles for you:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/dynamic-db-pool-config-with-sql-client/"&gt;Dynamic DB pool configuration with SQL client&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/26/modernizing-applications-apache-camel-javascript-and-red-hat-openshift"&gt;Modernizing applications with Camel, JS and OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases"&gt;Releases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always, the JBoss community has been releasing new version at a steady stream:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-1-2-final-released/"&gt;Quarkus 2.1.2 Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/08/kogito-1-9-1-released.html"&gt;Kogito 1.9.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org//2021/08/keycloak-1501-released.html"&gt;Keycloak 15.0.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another installment of our JBoss editorial! Stay safe and healthy in the meantime.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/vXVnnsrA7Uk" height="1" width="1" alt=""/&gt;</content><dc:creator>Romain Pelisse</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-08-11.html</feedburner:origLink></entry><entry><title type="html">Overview of making Hybrid Multi-cloud GitOps works</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ma9qpvkxcC8/overview-of-making-hybrid-multi-cloud.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/cqHOSS4n568/overview-of-making-hybrid-multi-cloud.html</id><updated>2021-08-10T19:10:00Z</updated><content type="html">Hybrid multi cloud can be a difficult, this is my study of a real customer use case on their journey using GitOps, multi cluster management system and securing dynamic infrastructure secrets.    Quick recap,  Last article I talked about the study I did among Red Hat customers that makes the jump towards deploying their workloads on hybrid and multi-cloud environments.  These articles are abstractions of the common generic components summarized according to the actual implementations.  To overcome the common obstacles of going hybrid and multi-cloud, such as finding talents with multi-cloud knowledge. Secure and protect across low trust networks or just day to day operation across the board. I have identify some solutions from the study, where I will be covering in the serie of articles: * *   * * IaC and GitOps, Infrastructure as code (IaC) allows the infrastructure to be more visible among teams through sharable files and scripts that contain how environments are set up and configured. We can also automate to speed up the provisioning and deployment process and avoid manual misconfiguration. GitOps takes it a step further, better managing the setting and configuration files into a versioned, centralized repository as the single source of truth. This allows better collaboration. At the same time, applying pipeline along with status check and testing, is perfect for introducing the continuous integration and continuous delivery (CI/CD) practice. This eliminates configuration drift, by constantly checking if the end environment is in-sync with the desired state defined in the repository.  How it works, First, we start with getting the management hub ready. (for more information, see my previous post.) In short, in the management hub, we will have Red Hat Advanced Cluster Management for Kubernetes (RHACM) installed. Where it is used for provisioning/patching/updating the OpenShift/Kubernetes clusters (This is the key to hybrid and multi cloud environments). On top of RHACM, OpenShift GitOps is also installed for declarative continuous delivery, where it watches the manifests in repositories and automatically(or manually after configuration) updates deployments to the desired state.  Here is the flow of infrastructure continuous delivery on hybrid and multi cloud:  1. Manifest and configuration are set as code template in the form of “Kustomization” yaml. It describes the end desire state of how the managed cluster is going to be like.  When done, it is pushed into the source control management repository with version assigned to each update.  2. OpenShift GitOps watches the repository and detects changes in the repository.   3. OpenShift GitOps creates/updates the manifest by creating Kuberenet objects on top of RHACM. 4. ACM provision/update/delete managed clusters and configuration according to the manifest. In the manifest, you can configure what cloud provider the cluster will be on, the name of the cluster, infra node details and worker node. Governance policy can also be applied as well as provision an agent in the cluster as the bridge between the control center and the managed cluster.  5. OpenShift GitOps will continuously watch between the code repository and status of the clusters reported back to RHACM. Any configuration drift or in case of any failure, it will automatically try to remediate by applying the manifest (Or showing alerts for manual intervention).  For application or updating during the continuous delivery, the process is very similar:  1. Developers have continuous integration pipelines to build application binary and images. Any changes to the infrastructure will also be pushed to the source control management repository. 2. OpenShift GitOps watches the repository and detects changes in the repository.   3. OpenShift GitOps create/update kustomize resources directly on to the managed clusters via the control plane 4. Change locally in the managed cluster. This concludes my study on the overall architecture of how GitOps works in Hybrid Multi-cloud environments. If you want to dive deeper into the technology, check out Ales Nosek’s youtube video, where in his video he will take you through step by step on how to do GitOps to manage your clusters. &lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ma9qpvkxcC8" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/cqHOSS4n568/overview-of-making-hybrid-multi-cloud.html</feedburner:origLink></entry><entry><title type="html">Kogito Task Deadlines</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lZAXRUuVI3I/kogito-task-deadlines.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2021/08/kogito-task-deadlines.html</id><updated>2021-08-10T18:27:58Z</updated><content type="html">At the beginning of the second decade of the twenty-first century we can still claim that smart humans are more clever than average machines. On the other hand, we probably have to admit that any machine is more reliable as a scheduler and reminder than an average human. So, when we face a task that requires human intervention in order to be completed, it feels natural to be able to set up a machine to periodically warn the human that his clever assistance is required to fulfill the task on time. That’s precisely the purpose of Kogito task deadlines functionality.  DEADLINE DESCRIPTION The basic idea behind deadlines is that when they expire, a notification is triggered. Hence, a deadline is composed of an expiration date, the type of occurrence and notification information.  The expiration date will either be an exact date (just one notification) or a period interval: bounded (by establishing a maximum number of notifications)  or unbounded (no limit of notifications). It is worth mentioning that when authoring the BPMN process, the human designer associates a deadline date to the task with the help of a graphical tool, but that internally the BPMN process is stored as a string following format. That’s the reason why the tool also allows advanced users, familiar with ISO 8601 syntax, to directly type it.  The deadline type can be “notStarted” or “notCompleted”. When the deadline date is reached, if type is “notStarted”, a notification is triggered if no phase transition has been performed over the task. “notCompleted” deadline types are triggered if the task is not completed. So we are essentially covering the two most common branches of human laziness: a task that has not been worked at all and a task that was started but left uncompleted.  The notification information is the data that will be included in the notification if it is eventually fired, which leads us to the question: what happens when a notification is triggered?. Kogito is handling the notification as a specific type of event: UserTaskDeadline. That event will contain task information plus the deadline notification information, which is a list of key-value pairs. These  values might contain MVEL expressions to be evaluated against the task information. Combining arbitrary information with the power of an expression language, the task notification event might be used for multiple purposes.  In fact, the way the task notification event is handled depends on the add-ons that are configured. By default Kogito does nothing, but it provides two addons to publish the event on a message broker: and . Once it is published, someone should consume the notification event to do something useful with it. Kogito provides another pair of add-ons to send an email using notification information included in the deadline: and . On summary, when task-notification and email addon are included in the project pom.xml for our target platform (Quarkus or Springboot) and a deadline is configured in the BPMN process, if a task is left unattended for a while, an email will be send. Let’s see how it works extending the approvals example from previous .  DEADLINE EXAMPLE.  As you probably recall, process defines a task called firstLineApproval. We are going to configure Kogito, using Quarkus as target platform, to send an email every minute till the firstLineApproval human task instance that is created at process startup is completed.  BPMN CONFIGURATION  The first thing to do is to update the bpmn process through the graphical editor. In the notification window associated with the firstLineApproval task, we can provide details about the deadline date plus the data needed to send an email. As previously mentioned in the introduction, although the window is designed for email notifications, notifications are generic, meaning that fields specified in the graphical interface are internally handled as a generic key value pair list. This will allow developing custom extensions different from the email addon we are going to use in this example.  As you can see in the screenshots, we are telling Kogito to fire a notification every minute till the task is completed. When the period expires, an email to will be sent, reminding that lazy user that something needs to be done with the task.  ADD-ON CONFIGURATION  Once bpmn is properly configured,  we still need to set up the addons. Approvals project already has a notification profile that includes these dependencies.  &lt;profile&gt; &lt;id&gt;notification&lt;/id&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;mail-quarkus-addon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;task-notification-quarkus-addon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-reactive-messaging-kafka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt; As you can see, we are selecting  task-notification-quarkus-addon, which uses messaging library to define a channel called kogito-deadline-events, where an event containing the notification data is published. Since we are going to configure that channel to use Kafka, we also include quarkus-smallrye-reactive-messaging-kafka dependency We configure the publishing channel to use Kafka by adding these properties to . mp.messaging.outgoing.kogito-deadline-events.connector=smallrye-kafka mp.messaging.outgoing.kogito-deadline-events.topic=kogito-deadline-events mp.messaging.outgoing.kogito-deadline-events.value.serializer=io.quarkus.kafka.client.serialization.ObjectMapperSerializer Once we have notification events being published into Kafka, we need to set up a listener that sends the email upon event reception. We do that by including mail-quarkus-addon into project pom. This add-on will listen for events on a smallrye channel named kogito-deadline-consumer and send an email using the information contained in the message received by that channel.  In order to setup the incoming channel, we need to mimic the configuration provided for the publishing channel. As you correctly guessed, we also do that in the same application.properties file. mp.messaging.incoming.kogito-deadline-consumer.connector=smallrye-kafka mp.messaging.incoming.kogito-deadline-consumer.topic=kogito-deadline-events mp.messaging.incoming.kogito-deadline-consumer.value.deserializer=org.kie.kogito.mail.DeadlineEventDeserializer Finally, we need to tell the mail addon the details of the smtp server to be used to send email. We do that by adding following properties to application.properties quarkus.mailer.host=localhost quarkus.mailer.port=25 quarkus.mailer.mock=false quarkus.mailer.ssl=false quarkus.mailer.start-tls=disabled In this particular example, since I’m using , I’m setting up a local . A complete configuration reference for quarkus mailer, the underlying library being used for the mail addon, can be found .  When all this configuration is in place, once the process instance is started, if nothing else is done, after a minute the following e-mail appears in my account.  CONCLUSION  In this post we have learnt how to set up Kogito to send an email when a task has not been completed for a while. We also learned that this is just the tip of the iceberg, because task deadline notifications can be used for many other purposes, only limited by the creativity of developers willing to implement more addons.  The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lZAXRUuVI3I" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/kogito-task-deadlines.html</feedburner:origLink></entry><entry><title type="html">Hybrid Multi-cloud dynamic security management</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ffGMCylyuDE/hybrid-multi-cloud-dynamic-security.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/v63zc5Bf-Rc/hybrid-multi-cloud-dynamic-security.html</id><updated>2021-08-10T14:29:00Z</updated><content type="html">HYBRID MULTI CLOUD CAN BE A DIFFICULT, THIS IS MY STUDY OF A REAL CUSTOMER USE CASE ON THEIR JOURNEY USING GITOPS, MULTI CLUSTER MANAGEMENT SYSTEM AND SECURING DYNAMIC INFRASTRUCTURE SECRETS. QUICK RECAP,  In my series of articles I went over the study I did among Red Hat customers that makes the jump towards deploying their workloads on hybrid and multi-cloud environments. These articles are abstractions of the common generic components summarized according to the actual implementations.  To overcome the common obstacles of going hybrid and multi-cloud, such as finding talents with multi-cloud knowledge. Secure and protect across low trust networks or just day to day operation across the board. I have identify some solutions from the study, where I will be covering in the serie of articles:  * *   * * DYNAMIC SECURITY MANAGEMENT, Kubernetes offers it’s own secret management control, although it’s sufficient for running a single cluster, but when you are trying to manage multiple sets of credentials and secure configurations, especially with the introduction of automated process and continuous delivery practice. We need a better way to securely and centrally manage these data.  HOW IT WORKS, Checkout my previous article on the setup of the hybrid and multi cloud environment, and if you are interested, another article on getting the GitOps works. But for now, we are going to assume we have a fleet of clusters deployed on top of multiple cloud vendors, and one in the local data center. All the infrastructure is set as code and stored in a source management system. Where our GitOps system constantly coverage the managed clusters with its desired state.  In order to setup a secure way to manage credentials and configuration cross clusters, we need two components,  * External Secret management in OpenShift/Kubernetes * Enable use of external secret management systems (like HashiCorp Vault in this case) to securely add secrets into the OpenShift platform.  * Hashicorp Vault   * Secure centralized store for dynamic infrastructure and application across clusters. For low trust networks between clouds and data centers.  This is how the two components work together to manage secret in dynamic infrastructure:  1. During setup, the token to securely access HashiCorp Vault is stored in Ansible Vault. It is encrypted to protect sensitive content. 2. Red Hat Advanced Cluster Management for Kubernetes (RHACM) allows us to have centralized control over the managing clusters. It acquires the token from Ansible Vault during install and distributes among the clusters.    3. To allow the cluster access to the external vault, we need to set up the external secret management. OpenShift Gitops is used to deploy the external secret object to a managed cluster.  4. External secret management fetches secrets from HashiCorp Vault using the token we created in step 2. And constantly watch for updates.  5. Secrets are created in each namespace, where applications can use.   This is how to manage dynamic infrastructure secret in a multi cluster and cloud environment. &lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ffGMCylyuDE" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/v63zc5Bf-Rc/hybrid-multi-cloud-dynamic-security.html</feedburner:origLink></entry><entry><title type="html">Four steps to author BPMN and DMN assets on gitpod.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zmLnvrJNDUk/four-steps-to-author-bpmn-and-dmn-assets-on-gitpod-io.html" /><author><name>Guilherme Caponetto</name></author><id>https://blog.kie.org/2021/08/four-steps-to-author-bpmn-and-dmn-assets-on-gitpod-io.html</id><updated>2021-08-09T19:31:10Z</updated><content type="html">In early 2021, , an open, flexible, and extensible cloud &amp;amp; desktop IDE platform, integrated support of VS Code custom editors API. It means that users would be able to run our extensions on this platform as well. Since then, we have been looking forward to seeing this feature enabled in . For those not familiar with gitpod.io, it basically provides a fully online development environment for you to work in your code on top of eclipse-theia. Learn more about Gitpod and about all its features. Online development environments are a recent trend in the industry and can potentially be a game-changer for developers. And you can leverage that to develop your Business Automation assets. Check out how to do that! Photo by on  1-) ACCESS YOUR CODE The first step is to go to your repository. Gitpod supports GitHub, GitLab and BitBucket. It gives you the flexibility to host your project wherever you want. In this post, I’ll be using , which contains a maven project created from the . GitHub repository example. 2-) START GITPOD.IO The second step is to add gitpod.io/# as prefix to your repository URL and simply open it. In my case, I’m accessing gitpod.io/#. The first time you access Gitpod, you’ll be asked to log in with GitHub, GitLab, or BitBucket. After doing so, Gitpod will automatically create a workspace for you and open your repository in the IDE. Starting up your workspace on gitpod.io. You can also install the Gitpod which will basically do Step 2 for you with a click of a button. Once you install the extension, the button will be available in your repository. Gitpod Chrome extension adds a button in your repository. 3-) INSTALL RED HAT BUSINESS AUTOMATION BUNDLE The third step is to install our extensions as you would normally do on VS Code. To do so, go to the Extensions menu in your newly created workspace and search for “Red Hat Business Automation Bundle”. This will automatically install our BPMN and DMN extensions. Installing the Red Hat Business Automation Bundle. 4-) VISUALIZE AND AUTHOR YOUR BPMN AND DMN ASSETS Now you are ready to visualize and author your BPMN and DMN assets! Visualizing and authoring BPMN and DMN assets. BONUS You can also leverage the workspace capabilities by triggering a build of your project in order to be confident that new changes are correct. To do so, simply use the provided terminal as you would use in VS Code. Since my repository has a maven project, I can run mvn clean install, which builds the project and runs the tests. Project built within Gitpod using the terminal. I can also start up Quarkus in development mode. To do so, I can run mvn clean package quarkus:dev -DskipTests on the terminal. Then, I can access the SwaggerUI to test my model. Remember, I didn’t install anything on my machine as all those things are in the cloud! Awesome, am I right? Starting up Quarkus in development mode and accessing the SwaggerUI. You can even set the service to be publicly available and share the SwaggerUI URL for others to try! And that’s all for today. Thanks for reading! &#x1f603; The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zmLnvrJNDUk" height="1" width="1" alt=""/&gt;</content><dc:creator>Guilherme Caponetto</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/four-steps-to-author-bpmn-and-dmn-assets-on-gitpod-io.html</feedburner:origLink></entry></feed>
